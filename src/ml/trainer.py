# src/ml/trainer.py
# Trainer script Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¡t hiá»‡n gian láº­n sá»­ dá»¥ng Autoencoder

import pandas as pd
import numpy as np
import glob
import os
import pickle
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# Import config paths tá»« project cá»§a báº¡n
from src.core.config import MODEL_PATH, SCALER_PATH

def train_model():
    print("ğŸ”„ Finding Dataset...")
    # Logic tÃ¬m file csv (Æ¯u tiÃªn Kaggle input, sau Ä‘Ã³ lÃ  local data)
    files = glob.glob("/kaggle/input/paysim1/*.csv") + glob.glob("data/*.csv")
    
    if not files:
        print("âŒ Dataset not found! Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n.")
        return

    print(f"âœ… Found dataset: {files[0]}")
    df = pd.read_csv(files[0])

    # --- 1. PREPROCESSING (Giá»‘ng trong notebook) ---
    print("ğŸ§¹ Cleaning & Preprocessing...")
    
    # Loáº¡i bá» cÃ¡c cá»™t khÃ´ng dÃ¹ng Ä‘á»ƒ train
    cols_to_drop = ['nameOrig', 'nameDest', 'isFlaggedFraud']
    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])
    
    # One-hot encoding cho cá»™t 'type'
    df = pd.get_dummies(df, columns=['type'], prefix='type')
    
    # Chuyá»ƒn vá» float
    df = df.astype(float)

    # Chá»‰ dÃ¹ng giao dá»‹ch bÃ¬nh thÆ°á»ng (Not Fraud) Ä‘á»ƒ train Autoencoder
    df_normal = df[df['isFraud'] == 0]
    
    # Bá» cá»™t label 'isFraud' vÃ  'step' khi Ä‘Æ°a vÃ o model
    drop_cols = ['isFraud', 'step']
    X_normal = df_normal.drop(columns=[c for c in drop_cols if c in df_normal.columns])

    # Chia táº­p Train/Test
    X_train, X_test = train_test_split(X_normal, test_size=0.2, random_state=42)
    
    print(f"ğŸ“Š Training on {len(X_train)} normal records...")

    # --- 2. SCALING ---
    print("âš–ï¸ Scaling data...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # LÆ°u Scaler Ä‘á»ƒ dÃ¹ng láº¡i khi predict
    # (Äáº£m báº£o thÆ° má»¥c tá»“n táº¡i)
    os.makedirs(os.path.dirname(SCALER_PATH), exist_ok=True)
    with open(SCALER_PATH, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"ğŸ’¾ Scaler saved at: {SCALER_PATH}")

    # --- 3. MODEL ARCHITECTURE (Autoencoder) ---
    print("ğŸ—ï¸ Building Autoencoder Model...")
    input_dim = X_train_scaled.shape[1]

    input_layer = Input(shape=(input_dim,))
    
    # Encoder
    encoder = Dense(8, activation="tanh")(input_layer)
    encoder = BatchNormalization()(encoder)
    latent_space = Dense(4, activation="tanh")(encoder) # Bottleneck
    
    # Decoder
    decoder = Dense(8, activation="tanh")(latent_space)
    output_layer = Dense(input_dim, activation="linear")(decoder)

    autoencoder = Model(inputs=input_layer, outputs=output_layer)
    autoencoder.compile(optimizer='adam', loss='mean_squared_error')

    # --- 4. TRAINING ---
    print("ğŸš€ Start Training...")
    callback = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
    
    autoencoder.fit(
        X_train_scaled, X_train_scaled,
        epochs=5, # Demo Ä‘á»ƒ nhanh, thá»±c táº¿ cÃ³ thá»ƒ tÄƒng lÃªn
        batch_size=2048,
        shuffle=True,
        validation_data=(X_test_scaled, X_test_scaled),
        callbacks=[callback],
        verbose=1
    )

    # --- 5. SAVING MODEL ---
    autoencoder.save(MODEL_PATH)
    print(f"âœ… Model saved at: {MODEL_PATH}")

    # LÆ°u láº¡i danh sÃ¡ch cá»™t training Ä‘á»ƒ lÃºc predict Ä‘áº£m báº£o Ä‘Ãºng thá»© tá»±
    # (Máº¹o nhá»: lÆ°u cÃ¡i nÃ y Ä‘á»ƒ trÃ¡nh lá»—i lá»‡ch cá»™t khi One-hot encoding)
    columns_path = os.path.join(os.path.dirname(MODEL_PATH), "model_columns.pkl")
    with open(columns_path, 'wb') as f:
        pickle.dump(X_train.columns.tolist(), f)
    print(f"â„¹ï¸ Model columns info saved at: {columns_path}")

if __name__ == "__main__":
    train_model()