# src/streaming/spark_job.py
# Spark Streaming Job ƒë·ªÉ ƒë·ªçc giao d·ªãch t·ª´ Kafka, d·ª± b√°o gian l·∫≠n v√† x·ª≠ l√Ω c·∫£nh b√°o



import sys
import os
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
import pandas as pd

sys.path.append(os.getcwd())

from src.ml.predictor import FraudPredictor
from src.core.config import KAFKA_BOOTSTRAP_SERVERS
from src.services.alert_service import AlertService # <--- Class m·ªõi

# C·∫•u h√¨nh Kafka Input
INPUT_TOPIC = "raw_transactions"

# Kh·ªüi t·∫°o Global Services (Lazy Loading)
predictor = None
alert_service = None

def get_services():
    global predictor, alert_service
    if predictor is None:
        print("üõ†Ô∏è Initializing Services...")
        predictor = FraudPredictor()
        alert_service = AlertService()
    return predictor, alert_service

def process_batch(df_batch, epoch_id):
    if df_batch.count() == 0: return

    start_time = time.time()
    
    # 1. Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu
    rows = df_batch.collect()
    pdf = pd.DataFrame(rows, columns=df_batch.columns)
    
    # 2. L·∫•y Services
    model, alerter = get_services()
    
    try:
        # 3. AI D·ª± b√°o
        print(f"üì¶ Batch {epoch_id}: Predicting {len(pdf)} tx...")
        result_df = model.predict(pdf)
        
        # 4. Giao vi·ªác x·ª≠ l√Ω k·∫øt qu·∫£ cho AlertService
        # (Spark Job kh√¥ng c·∫ßn bi·∫øt AlertService l√†m g√¨ b√™n trong)
        alerter.process_alerts(result_df)

    except Exception as e:
        print(f"‚ùå Batch Processing Error: {e}")
        import traceback
        traceback.print_exc()

    print(f"‚úÖ Batch {epoch_id} done in {time.time() - start_time:.2f}s")

def start_streaming():
    print("üöÄ Starting Spark Streaming (Clean Architecture)...")
    
    spark = SparkSession.builder \
        .appName("FraudGuard_Pro") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0") \
        .getOrCreate()
    spark.sparkContext.setLogLevel("WARN")

    # Schema
    schema = StructType([
        StructField("step", IntegerType()),
        StructField("type", StringType()),
        StructField("amount", DoubleType()),
        StructField("nameOrig", StringType()),
        StructField("oldbalanceOrg", DoubleType()),
        StructField("newbalanceOrig", DoubleType()),
        StructField("nameDest", StringType()),
        StructField("oldbalanceDest", DoubleType()),
        StructField("newbalanceDest", DoubleType()),
        StructField("isFraud", IntegerType()),
        StructField("isFlaggedFraud", IntegerType())
    ])

    # Read Kafka
    df_kafka = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS[0]) \
        .option("subscribe", INPUT_TOPIC) \
        .option("startingOffsets", "latest") \
        .load()

    df_parsed = df_kafka.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

    query = df_parsed.writeStream \
        .foreachBatch(process_batch) \
        .trigger(processingTime='2 seconds') \
        .start()

    query.awaitTermination()

if __name__ == "__main__":
    start_streaming()