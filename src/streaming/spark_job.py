# src/streaming/spark_job.py
# Spark Structured Streaming Job ƒë·ªÉ ƒë·ªçc d·ªØ li·ªáu giao d·ªãch t·ª´ Kafka,

import sys
import os
import json
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, struct, to_json
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
import pandas as pd
from kafka import KafkaProducer

# Import module d·ª± b√°o c·ªßa ch√∫ng ta
# Th√™m ƒë∆∞·ªùng d·∫´n root v√†o path ƒë·ªÉ Spark t√¨m th·∫•y src
sys.path.append(os.getcwd())
from src.ml.predictor import FraudPredictor
from src.core.config import KAFKA_BOOTSTRAP_SERVERS, JDBC_DRIVER_PATH

# C·∫•u h√¨nh Kafka Topic
INPUT_TOPIC = "raw_transactions"
OUTPUT_TOPIC_PREDICTIONS = "fraud_predictions"
OUTPUT_TOPIC_ALERTS = "fraud_alerts"

# Kh·ªüi t·∫°o Global Predictor (Load model 1 l·∫ßn duy nh·∫•t)
# L∆∞u √Ω: Trong m√¥i tr∆∞·ªùng Spark Cluster th·∫≠t, n√™n d√πng mapPartitions. 
# V·ªõi Demo single node, global var l√† ·ªïn.
predictor = None

def get_predictor():
    global predictor
    if predictor is None:
        print("Lazy loading Predictor...")
        predictor = FraudPredictor()
    return predictor

def write_to_kafka(topic, data):
    try:
        producer = KafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        for record in data:
            producer.send(topic, record)
        producer.flush()
        producer.close()
    except Exception as e:
        print(f"‚ùå Kafka Write Error: {e}")

def process_batch(df_batch, epoch_id):
    """
    H√†m x·ª≠ l√Ω t·ª´ng l√¥ d·ªØ li·ªáu (Batch) t·ª´ Kafka
    """
    if df_batch.count() == 0:
        return

    start_time = time.time()
    
    # 1. Chuy·ªÉn Spark DataFrame -> Pandas DataFrame ƒë·ªÉ ƒë∆∞a v√†o Model AI
    # (V√¨ TensorFlow ch·∫°y tr√™n Pandas/Numpy t·ªët h∆°n)
    rows = df_batch.collect()
    pdf = pd.DataFrame(rows, columns=df_batch.columns)
    
    print(f"üì¶ Batch {epoch_id}: ƒêang x·ª≠ l√Ω {len(pdf)} giao d·ªãch...")

    # 2. G·ªçi Model d·ª± b√°o
    model = get_predictor()
    try:
        # H√†m predict tr·∫£ v·ªÅ DF c√≥ th√™m c·ªôt 'anomaly_score' v√† 'is_fraud_prediction'
        result_df = model.predict(pdf)
        
        # Th√™m timestamp x·ª≠ l√Ω
        result_df['processed_at'] = time.strftime("%Y-%m-%d %H:%M:%S")
        
        # 3. G·ª≠i K·∫æT QU·∫¢ D·ª∞ B√ÅO v√†o topic 'fraud_predictions' (ƒê·ªÉ Dashboard v·∫Ω bi·ªÉu ƒë·ªì)
        records = result_df.to_dict(orient='records')
        write_to_kafka(OUTPUT_TOPIC_PREDICTIONS, records)
        
        # 4. L·ªçc v√† g·ª≠i C·∫¢NH B√ÅO v√†o topic 'fraud_alerts'
        # Ch·ªâ l·∫•y nh·ªØng c√°i ƒë∆∞·ª£c model d·ª± ƒëo√°n l√† Fraud (True)
        fraud_alerts = result_df[result_df['is_fraud_prediction'] == True]
        
        if not fraud_alerts.empty:
            alert_records = fraud_alerts.to_dict(orient='records')
            write_to_kafka(OUTPUT_TOPIC_ALERTS, alert_records)
            print(f"üö® PH√ÅT HI·ªÜN {len(fraud_alerts)} GIAO D·ªäCH GIAN L·∫¨N! ƒê√£ g·ª≠i c·∫£nh b√°o.")
            
            # (T√πy ch·ªçn) In ra m√†n h√¨nh console v√†i d√≤ng ƒë·ªÉ debug
            print(fraud_alerts[['amount', 'anomaly_score']].head())

    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh d·ª± b√°o: {e}")
        import traceback
        traceback.print_exc()

    print(f"‚úÖ Ho√†n th√†nh Batch {epoch_id} trong {time.time() - start_time:.2f}s")

def start_streaming():
    print("üöÄ ƒêang kh·ªüi ƒë·ªông Spark Streaming Job...")
    
    # C·∫•u h√¨nh Spark v·ªõi g√≥i Kafka
    spark = SparkSession.builder \
        .appName("FraudDetectorSystem") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")

    # Schema ƒë√∫ng v·ªõi d·ªØ li·ªáu Paysim1
    schema = StructType([
        StructField("step", IntegerType()),
        StructField("type", StringType()),
        StructField("amount", DoubleType()),
        StructField("nameOrig", StringType()),
        StructField("oldbalanceOrg", DoubleType()),
        StructField("newbalanceOrig", DoubleType()),
        StructField("nameDest", StringType()),
        StructField("oldbalanceDest", DoubleType()),
        StructField("newbalanceDest", DoubleType()),
        StructField("isFraud", IntegerType()),
        StructField("isFlaggedFraud", IntegerType())
    ])

    # ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka 'raw_transactions'
    df_kafka = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS[0]) \
        .option("subscribe", INPUT_TOPIC) \
        .option("startingOffsets", "latest") \
        .load()

    # Parse JSON t·ª´ Kafka value
    df_parsed = df_kafka.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

    # B·∫Øt ƒë·∫ßu lu·ªìng x·ª≠ l√Ω
    query = df_parsed.writeStream \
        .foreachBatch(process_batch) \
        .trigger(processingTime='2 seconds') \
        .start()

    print(f"üì° ƒêang l·∫Øng nghe topic '{INPUT_TOPIC}'...")
    query.awaitTermination()

if __name__ == "__main__":
    start_streaming()